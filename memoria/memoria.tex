\documentclass[a4paper,11pt]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{booktabs}  
\usepackage{graphicx} 
\usepackage{listings}
\usepackage{amsmath,amsthm,amssymb}

\lstset{%
backgroundcolor=\color{cyan!10},
basicstyle=\ttfamily,
numbers=left,numberstyle=\scriptsize
}
\setlength{\parindent}{0cm}
\usepackage[wby]{callouts}
\usepackage{hyperref}

\title{Memoria Proyecto Final}
\author{Álvaro Beltrán y Yábir García}
\begin{document}

\maketitle

\begin{figure}[h]
\includegraphics[scale=0.3]{UGR}
\centering
\end{figure}

\newpage

\renewcommand*\contentsname{Índice}
\tableofcontents

\newpage

\section{Definición del problema a resolver y enfoque elegido}

Para nuestro proyecto nos hemos decantado por trabajar con el conjunto de datos
\textit{Adult Census Income} recogido por Ronny Kohavi y Berry Becker. 

Se nos presenta un conjunto muestras para las que se han recogido distintos
parámetros que pueden afectar a la cantidad de dinero que se ingresa. Las
variables, las cuales se analizarán posteriormente, abarcan desde la ] edad
hasta el país de origen y el problema consiste en determinar cuando una muestra
va superar las 50000 unidades monetarias de ingresos y cuando se va a mantener
por debajo.

Estamos ante un problema de aprendizaje supervisado en el ámbito de la
clasificación binaria. En nuestro caso los elementos que intervienen en el
aprendizaje son 

\begin{itemize}
    \item $X$: Conjunto de datos relativos a una persona que pueden tener
    repercusión en el dinero que gane. Se trata de un vector que mezcla
    variables categóricas con variables reales.
    \item $y$: Es un valor en el conjunto $\{0,1\}$ que nos indica si los
    ingresos superan las 50000 unidades monetarias o no.
    \item $f$: Función que nos relaciona el conjunto $X$ con $y$ de manera que a
    cada muestra le asigna su categoria de ingresos.
\end{itemize}

En este problema es dicha función $f$ la que queremos aproximar. Para ello vamos
a basar nuestro estudio en aplicar el conocimiento adquirido durante la
asignatura utilizando métodos lineales y no lineales.

\section{Obtención de los datos}

Para la obtención de los datos hemos utilizado el repositorio público de
\textit{UCI}, más concretamente la url
\href{https://archive.ics.uci.edu/ml/machine-learning-databases/adult/}{https://archive.ics.uci.edu/ml/machine-learning-databases/adult/}.
Podemos apreciar como hay distintos archivos entre los que se encuentran los
datos y una descripción. 

Los datos aparecen separados en dos archivos distintos, uno para training y otro
para test. Descargamos ambos ficheros (\textit{adult.data}, \textit{adult.test})
y en nuestro trabajo hemos decidido combinarlos para realizar una separación
propia.

\section{Argumentos a favor de la elección de los modelos.}

En cuanto a los modelos lineales elegidos nos hemos decantado por Regresión
Logística y Perceptron, debido a que son dos modelos que hemos estudiado en
clase para ejemplos de clasificación binaria, como es nuestro caso. \\

Además parece interesante probar con el Perceptron puesto que también vamos a
usar el perceptron multicapa (MLP). Y puesto que Regresión Logística ha dado tan
buenos resultados en las prácticas de esta asignatura y suele funcionar muy bien
en clasificación parece indispensable probar este modelo.\\

De los modelos no lineales propuestos hemos elegido Perceptron Multicapa, Random Forest y SVD.\\

Vamos a usar Ramdom Forest por que suele ser bueno para clasificación, a parte de que es poco sensible a cambios en el conjunto de train (vamos a usar validación cruzada). Además, de los algoritmos actuales ninguno le supera en precisión y nuestros datos no están correlados factor que mejora su rendimiento.\\

Perceptron Multicapa.\\

SVC.\\

\section{Preprocesado de los datos}

En primer lugar vamos a analizar el típo de variables con los que contamos. Este
análisis lo recogemos en la siguiente tabla

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    Variable & Tipo de variable & Número de categorías\\ \hline
    age & continua & - \\ \hline
    workclass & categórica & 8 \\ \hline
    fnlwgt & continua & - \\ \hline
    education & categórica & 16 \\ \hline
    education-num & continua & - \\ \hline
    marital-status & categórica & 7 \\ \hline
    occupation & categórica & 14 \\ \hline
    relationship & categórica & 6 \\ \hline
    race & categórica & 5 \\ \hline
    sex & categórica & 2 \\ \hline
    capital-gain & continua & - \\ \hline
    capital-loss & continua & - \\ \hline
    hours-per-week & continua & - \\ \hline
    native-country & categórica & 41 \\ \hline
\end{tabular}
\caption{Variables estudiadas en las muestras, su tipo y la cantidad de categorías si procede.}
\end{table}

\subsection{Codificación de las variables}

Tenemos un conjunto de variables en las que se combinan tipos continuos y
categóricos por lo que nos va a resultar necesario convertir los datos
categóricos a valores reales. Para ello hemos optado por realizar para cada
variable de tipo categórico un vector de ceros y unos que representa con un uno
la pertenencia a una categoría concreta. Por ejemeplo en el caso de la variable
\textit{sex} que cuenta con dos categorías, $(1,0)$ representaría la pertenencia
al sexo femenino y $(0,1)$ al sexo masculino. En esta codificación de los datos
no es posible obtener un vector con más de un uno ya que en nuestros datos no se
pertenece a dos categorías de manera simultanea.

En nuestro caso nos hemos decantado por utilizar el método de pandas
\textit{get\_dummies}, que nos proporciona este comportamiento incluyendo la
codificación como variables en nuestro \textit{dataframe} de trabajo.

\subsection{Valoración del interes de las variables y selección de un subconjunto}

En principio, tras leer la descripción de los atributos estudiados. Solo vemos
dos variables que no aportan información al problema. La primera es
"Education-num" que es una representación numérica del atributo education, no
aporta nada pues hemos tomado la decisión de dividir cada variable categórica en
una serie de variables donde asignamos 1 si es de un determinado tipo del
dominio de la variable o 0 si no es de ese tipo. La segunda variable es "fnlwgt"
que determina el número de personas representadas con esa instancia, entendemos
que esa variable no aporta información de fuera de la muestra y por lo tanto no
es interesante.

\subsection{Normalización de las variables}

Los rangos en los que se mueven las variables son muy dispares y normalmente con
valores muy distantes por lo que hemos visto conveniente realizar una
normalización de las variables continuas. Para ello hemos aplicado
\textit{StandardScaler} de sklearn de manera que transforma la variables
dejandolas con medio 0 y varianza 1. De esta forma  todas las variables se
encuentran centradas entorno al mismo valor y en un rango similar. 

Esto es un paso muy importante ya que afecta directamente al funcionamiento 
de los algoritmos que se van a desarrollar.

\subsection{Valores perdidos}

En cuanto a los valores perdidos hemos seguido las directrices propuestas en el enunciado, hemos eliminado las instancias cuyos valores perdidos superaran el 10$\%$ (2 atributos con valores perdidos). Una vez eliminadas estas instancias hemos rellenado los valores perdidos restantes con la multinomial asignando a cada posible valor del domino del atributo una probabilidad dependiendo de la cantidad de veces que aparecen en las instancias. Para ello, hemos usado la funcion choice de la libreria random, determinando el dominio y las probabilidades con las que aparecen cada objeto del dominio.



\subsection{Conjunto de training y test}

En los datos extraidos se obitienen dos conjuntos de datos, uno pensado para
training y otro para test. Se nos indica además que en cada uno de estos
conjuntos encontramos un $75\%$ de muestras clasificadas con ingresos inferiores
a las 50000 u.m. y el $25\%$ con ingresos superiores a dicho valor. 

Hemos considerado conveniente realizar una separación de los datos en la que las
clases esten balanceadas ya que es un aspecto importante en los algoritmos de
clasificación y que puede afectar a la misma. 

Respecto a la separación de los datos, hemos decidido realizar una separación
$80-20$ para training y test respectivamente.

No hemos realizado una separación para datos de validación ya que nos hemos
decantado, como se describe en secciones posteriores, por un sistema de elección
de hiperparámetros basado en validación cruzada.

\section{Regularización}

En el caso del modelo lineal nos hemos decantado por utilizar regularización
basada en la norma 1, \textit{l1}, ya que hemos encontrado que no existe una
alta correlación lineal entre las variables y, con la introducción de las
variables \textit{dummies}, creemos que es la mejor elección. 

Para el algoritmo de random forest no se utiliza de manera implicita ningún
factor de regularización.

En el caso de SVC, el único parámetro que se puede moficiar es el coeficiente de
regularización, para el cual se ha aplicado un criterio de búsqueda en la
elección de hiperparámetros, pero no se puede elegir la métrica con la que se
regulariza.


\section{Justificación de la función de pérdida usada.}

En un principio pensamos en usar exactitud (Accuracy), pero esta métrica da no funciona bien cuando las clases están desbalanceadas como es en este caso. Así que, es muy fácil acertar diciendo que dicha predicción está en la clase más probable. Para problemas con clases desbalanceadas es mucho mejor usar precisión. Esta métrica da una mejor idea de la calidad del modelo. 
\begin{align*}
\frac{TP}{TP+FP}
\end{align*}
\textbf{(revisar esto de abajo)}\\
Además, con esta métrica conseguimos estudiar el porcentaje de casos positivos bien clasificados, asumiendo que en los casos negativos pasa lo mismo.



\section{Estimación de los hiperparámetros.}

%rellenar hiperparametros de los otros dos

Para los modelos estudiados anteriormente hemos seleccionado una serie de
parámetros y vamos a hacer inferencia sobre otros:

\begin{itemize}

\item \textbf{Regresión Logística}. Hemos decidido usar el solver lbfgs que es newton pero con mejoras en memoria para reducir el tiempo, vamos a usar este porque newton adapta la tasa de aprendizaje según le convenga y por tanto es más eficaz. Usamos penalización L1 ya que usamos Lasso. Y hacemos inferencia sobre los parámetros C y tol. El parámetro C es el coeficiente que acompaña a la penalización y tol es la tolerancia usada en el modelo.

\item \textbf{Perceptron}. Hemos prefijado la máximas iteraciones a 2000 por que probando con menos no acababa y hemos decidido que baraje los datos en cada iteración del perceptron como hemos visto en teoría. Hacemos inferencia sobre los parámetros alpha y tol. El parámetro alpha es el coeficiente que acompaña a la penalización (elegida la que viene por defecto) y tol es la tolerancia usada en el modelo.

\item \textbf{Ramdom Forest}. Vamos a usar bootstrap porque es capaz de medir la incertidumbre de nuestro modelo mediante una técnica de reelección de muestras. Para determinar las máximas características del árbol vamos a usar la raíz cuadrada por que hay evidencias empíricas de que es el mejor. Hemos hecho inferencia sobre el criterio de selección, sobre si elegir entropy o gini. 

\item \textbf{MLPClasiffier} (Perceptron Multicapa).Hemos decidido usar el solver lbfgs que es newton pero con mejoras en memoria para reducir el tiempo, al igual que hicimos en Regresión Logística. También hemos fijado las iteraciones máximas a 20000 por que si no, no conseguía converger. Además, hacemos inferencia sobre los parámetros alpha (ya explicado antes) y función de activación. Para la función de activación damos tres opciones, 'logistic', 'tanh', 'relu': $ \frac{1}{1 + exp(-x)} , tanh(x), max(0, x)$ respectivamente.

\item \textbf{SVC}. Para este modelo hemos fijado los parámetros kernel y gamma a rbf y scale respectivamente. Elegimos rbf por que es un kernel no lineal que no añade demasiada complejidad y \textbf{ scale por que }. Y hacemos inferencia sobre el parámetro C que ya hemos explicado en Regresión logística.

\end{itemize}

\section{Conclusiones}

Aquí decir que lineal good y que no hay nada que envidiar

%comparación entre modelos lineales y no lineales
%los resultados
%las gráficas 
%confusion matrix
%ideonidad de los modelos


%cosas que no se rellenar: porque gamma=scale, porque hemos decidido usar MLP y SVC
%revisar métrica


\end{document}
